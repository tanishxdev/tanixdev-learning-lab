

# MODULE 1.2 — Prompt Engineering

## 39. Multi-Step Prompting (Breaking One Big Task into Controlled Steps)

This topic explains **how to make complex tasks reliable**.

After this:

* large prompts stop failing randomly
* reasoning becomes controllable
* agent workflows start to make sense
* debugging becomes possible

---

## 1. Concept (Precise Definition)

**Multi-step prompting** means:

> **Decomposing a complex task into multiple smaller prompts, where each step has a clear goal and controlled output.**

Instead of asking the model to do everything at once, you guide it step by step.

---

## 2. Why Single-Step Prompts Fail

A single large prompt forces the model to:

* understand the task
* plan the solution
* reason correctly
* format output
* respect constraints

All in **one token stream**.

This often leads to:

* partial reasoning
* ignored constraints
* hallucinations
* format breakage

---

## 3. The Core Idea (Mental Model)

Replace this:

```
Big task → one prompt → hope
```

With this:

```
Step 1 → output
Step 2 → output
Step 3 → output
Final answer
```

Each step reduces uncertainty.

---

## 4. When Multi-Step Prompting Is Necessary

Use multi-step prompting when:

1. Task has multiple phases
2. Accuracy matters
3. Output must be structured
4. Reasoning is non-trivial
5. You plan to build agents

For simple tasks, it is unnecessary.

---

## 5. Example: One-Step vs Multi-Step (Conceptual)

### One-step (fragile)

```
Read this article, extract key points, summarize them, and output JSON.
```

This mixes:

* extraction
* reasoning
* formatting

High failure rate.

---

### Multi-step (robust)

Step 1: Extract key points
Step 2: Summarize key points
Step 3: Format as JSON

Each step is simple and controlled.

---

## 6. JavaScript Practice — Multi-Step Prompting (Groq)

### Step 1: Extract key points

```js
import groq from "../../../src/utils/groqClient.js";

async function extractPoints(text) {
  const res = await groq.chat.completions.create({
    model: "llama-3.1-8b-instant",
    temperature: 0,
    messages: [
      {
        role: "user",
        content: `Extract the key points from the following text:\n${text}`,
      },
    ],
  });

  return res.choices[0].message.content;
}
```

---

### Step 2: Summarize extracted points

```js
async function summarize(points) {
  const res = await groq.chat.completions.create({
    model: "llama-3.1-8b-instant",
    temperature: 0,
    messages: [
      {
        role: "user",
        content: `Summarize the following points in simple language:\n${points}`,
      },
    ],
  });

  return res.choices[0].message.content;
}
```

---

### Step 3: Format as JSON

```js
async function formatAsJSON(summary) {
  const res = await groq.chat.completions.create({
    model: "llama-3.1-8b-instant",
    temperature: 0,
    messages: [
      {
        role: "user",
        content: `
Return ONLY valid JSON.
Schema:
{
  "summary": "string"
}

Content:
${summary}
`,
      },
    ],
  });

  return JSON.parse(res.choices[0].message.content);
}
```

---

### Orchestrator

```js
async function run() {
  const text =
    "Binary search is an efficient algorithm for finding an item in a sorted list by repeatedly dividing the search interval in half.";

  const points = await extractPoints(text);
  const summary = await summarize(points);
  const result = await formatAsJSON(summary);

  console.log(result);
}

await run();
```

This is **production-style prompting**.

---

## 7. Why This Works Better

Multi-step prompting:

1. Reduces cognitive load per step
2. Narrows token probability space
3. Makes failures observable
4. Allows retries at specific steps

You can debug **which step failed**, not guess.

---

## 8. Multi-Step Prompting vs Chain-of-Thought

Do not confuse these.

* Chain-of-thought → internal reasoning
* Multi-step prompting → external orchestration

Multi-step prompting:

* happens in your code
* is explicit
* is debuggable

It is safer and more controllable.

---

## 9. Error Handling Becomes Possible

Example:

* Step 1 fails → retry extraction
* Step 2 fails → re-summarize
* Step 3 fails → fix JSON

You do not need to rerun everything.

This is critical for **agents and workflows**.

---

## 10. Cost Trade-Off (Be Aware)

Multi-step prompting:

* increases number of API calls
* increases latency
* increases total cost

But it:

* reduces retries
* reduces hallucinations
* increases reliability

In production, reliability usually wins.

---

## 11. Common Beginner Mistakes

1. Splitting steps without purpose
2. Carrying unnecessary text between steps
3. Forgetting to control temperature
4. Not validating intermediate outputs
5. Making steps too large

Each step should do **one thing**.

---

## 12. Mental Design Rule (Memorize)

If a prompt contains:

```
and
then
also
after that
```

It probably needs to be **multi-step**.

---

## 13. Where This Fits in the Roadmap

```
MODULE 1.2 — Prompt Engineering
31. Instruction prompting     Completed
32. Role prompting            Completed
33. Chain-of-thought          Completed
34. Zero-shot                 Completed
35. Few-shot                  Completed
36. Safety rewriting          Completed
37. JSON mode                 Completed
38. Injection prevention      Completed
39. Multi-step prompting      You are here
40. Prompt debugging          Next
```

---

## 14. One-Line Truth (Lock It)

> **Complex behavior emerges from simple, controlled steps — not from one giant prompt.**

---