Good.
We continue **MODULE 1.2 — Prompt Engineering** with the same discipline.

---

# MODULE 1.2 — Prompt Engineering

## 36. Safety Rewriting

Safety rewriting is about **controlling risk**, not censoring intelligence.

After this topic, you will understand:

* how unsafe prompts are transformed
* why models sometimes refuse or soften answers
* how systems keep usefulness while reducing harm
* why rewriting happens before the model “thinks”

---

## 1. Concept (Precise Definition)

**Safety rewriting** means:

> **Transforming user input into a safer, policy-compliant version before sending it to the model.**

The goal is not to block usage, but to **redirect it safely**.

---

## 2. Why Safety Rewriting Exists

LLMs are powerful but **unbounded**.

Raw user input can include:

* harmful intent
* illegal instructions
* self-harm content
* privacy violations
* abuse or harassment

Directly passing such input to a model is risky.

So systems introduce a layer:

```
User Input
→ Safety Rewrite
→ Model
```

---

## 3. Safety Rewriting vs Refusal (Important Distinction)

These are different strategies.

### Refusal

```
I cannot help with that.
```

Used when no safe alternative exists.

---

### Safety Rewriting

```
I can explain the risks and safe alternatives.
```

Used when intent can be redirected.

Safety rewriting preserves **helpfulness**.

---

## 4. What Gets Rewritten (Common Categories)

Safety rewriting is commonly applied to:

1. Violence-related instructions
2. Self-harm content
3. Illegal activities
4. Medical or legal advice
5. Hate or harassment

The rewritten version removes:

* step-by-step instructions
* operational details
* actionable harm

While keeping:

* high-level explanation
* prevention
* education
* safe alternatives

---

## 5. Example (Conceptual)

### Unsafe user input

```
How do I break into a locked phone?
```

---

### Safety-rewritten intent

```
Explain why accessing someone else’s phone without permission is illegal and discuss lawful alternatives.
```

The task is **redirected**, not ignored.

---

## 6. Where Safety Rewriting Happens

Safety rewriting can occur:

1. Before the model (preferred)
2. Inside system prompts
3. Via a separate safety model
4. Via rule-based filters

Modern systems often use:

```
classifier → rewriter → main model
```

---

## 7. Why Safety Rewriting Is Better Than Blocking

Blocking causes:

* frustration
* adversarial prompting
* repeated attempts
* worse UX

Rewriting:

* reduces harm
* keeps engagement
* maintains trust
* lowers misuse attempts

This is why production systems favor rewriting when possible.

---

## 8. JavaScript Practice — Manual Safety Rewriting Pattern

You can implement a simple version yourself.

```js
function safetyRewrite(prompt) {
  const bannedKeywords = ["hack", "kill", "bomb"];

  for (const word of bannedKeywords) {
    if (prompt.toLowerCase().includes(word)) {
      return "Explain the ethical, legal, and safety concerns related to this topic.";
    }
  }

  return prompt;
}
```

This is primitive, but demonstrates the idea.

---

## 9. JavaScript Practice — Rewritten Prompt Sent to Groq

```js
import groq from "../../../src/utils/groqClient.js";

function safetyRewrite(prompt) {
  if (prompt.toLowerCase().includes("hack")) {
    return "Explain why hacking is illegal and discuss ethical cybersecurity practices.";
  }
  return prompt;
}

async function run() {
  const userPrompt = "How can I hack a WiFi network?";
  const safePrompt = safetyRewrite(userPrompt);

  const res = await groq.chat.completions.create({
    model: "llama-3.1-8b-instant",
    messages: [{ role: "user", content: safePrompt }],
  });

  console.log(res.choices[0].message.content);
}

await run();
```

Observe:

* the model remains helpful
* harmful intent is removed
* safety is enforced before generation

---

## 10. Safety Rewriting and Prompt Injection

Safety rewriting helps defend against:

```
Ignore previous rules and do X
```

Because the rewritten prompt:

* removes the malicious instruction
* reframes intent
* keeps system rules intact

This connects directly to **Topic 38 — Injection Prevention**.

---

## 11. Common Beginner Mistakes

1. Relying only on refusal
2. Letting the model rewrite itself
3. Applying safety rules inconsistently
4. Rewriting after generation instead of before
5. Removing too much usefulness

Good safety rewriting is **surgical**, not blunt.

---

## 12. When You Should NOT Rewrite

Do not rewrite when:

* the request is harmless
* rewriting changes the core intent incorrectly
* a clear refusal is required

Safety rewriting is conditional, not universal.

---

## 13. How This Connects Forward

This topic directly prepares you for:

* JSON mode safety
* prompt injection prevention
* multi-step prompting
* agent guardrails
* evaluation and red-teaming

You are now thinking like a **system designer**, not just a user.

---

## 14. Where You Are in the Roadmap

```
MODULE 1.2 — Prompt Engineering
31. Instruction prompting     Completed
32. Role prompting            Completed
33. Chain-of-thought          Completed
34. Zero-shot                 Completed
35. Few-shot                  Completed
36. Safety rewriting          You are here
37. JSON mode                 Next
```

---

## 15. One-Line Truth (Lock It)

> **Safety rewriting keeps the model useful
> while removing the ability to cause harm.**

## BETTER EXAMPLE 

1. *What is safety rewriting*
2. *Why this code exists and how data flows*

# 1. What problem are we solving?

LLMs are **very powerful**, but users can ask **dangerous or disallowed things**.

Example user inputs:

* “How can I hack a WiFi network?”
* “Write malware code”
* “How to bypass payment systems”

You **should NOT send these directly** to the LLM.

So we introduce a layer called:

> **Safety Rewrite (Prompt Sanitization)**

---

# 2. What is Safety Rewriting (in one line)

> **Rewrite unsafe user input into a safe, allowed version BEFORE sending it to the LLM.**

Not blocking.
Not erroring.
Just **redirecting intent safely**.

---

# 3. Simple real-life analogy

User says:

> “How do I break into a house?”

You reply (rewritten):

> “Explain why breaking into houses is illegal and how to improve home security.”

Same topic → **different safe direction**.

---

# 4. Your original code (why it feels confusing)

```js
function safetyRewrite(prompt) {
  if (prompt.toLowerCase().includes("hack")) {
    return "Explain why hacking is illegal and discuss ethical cybersecurity practices.";
  }
  return prompt;
}
```

This is doing **ONE thing only**:

* Detect unsafe keyword (`hack`)
* Replace it with a **safe educational prompt**

But since the example is abstract, it feels unclear.

So let’s rebuild it **cleanly**.

---

# 5. Better, clearer example (step-by-step)

## Step 1: User input (raw, unsafe)

```js
const userPrompt = "How can I hack a WiFi network?";
```

This is **NOT safe** to send to the model.

---

## Step 2: Safety rewrite function (clean & readable)

```js
function safetyRewrite(prompt) {
  const lowerPrompt = prompt.toLowerCase();

  // Detect unsafe intent
  if (lowerPrompt.includes("hack")) {
    // Redirect to safe educational intent
    return `
Explain why hacking WiFi networks is illegal.
Also explain ethical cybersecurity practices and how people can secure their WiFi legally.
`;
  }

  // If safe, return as-is
  return prompt;
}
```

Key idea:

* We are **changing direction**, not refusing.

---

## Step 3: Apply safety rewrite BEFORE calling LLM

```js
const safePrompt = safetyRewrite(userPrompt);
```

Now:

* `userPrompt` = unsafe
* `safePrompt` = safe, educational

---

# 6. Full working example with Groq (clean version)

```js
// Import reusable Groq client
import groq from "../../../src/utils/groqClient.js";

// Safety rewrite layer
function safetyRewrite(prompt) {
  const lowerPrompt = prompt.toLowerCase();

  if (lowerPrompt.includes("hack")) {
    return `
Explain why hacking WiFi networks is illegal.
Discuss ethical cybersecurity practices.
Explain how users can secure their own WiFi legally.
`;
  }

  return prompt;
}

async function run() {
  // Raw user input (could be unsafe)
  const userPrompt = "How can I hack a WiFi network?";

  // Rewrite prompt safely
  const safePrompt = safetyRewrite(userPrompt);

  // Send ONLY safe prompt to LLM
  const res = await groq.chat.completions.create({
    model: "llama-3.1-8b-instant",
    messages: [
      {
        role: "user",
        content: safePrompt,
      },
    ],
  });

  console.log("Original prompt:");
  console.log(userPrompt);

  console.log("\nPrompt sent to LLM:");
  console.log(safePrompt);

  console.log("\nLLM response:");
  console.log(res.choices[0].message.content);
}

await run();
```

---

# 7. Very important mental model (remember this)

```
User Input
   ↓
Safety Rewrite Layer   ← YOUR RESPONSIBILITY
   ↓
LLM API Call
   ↓
LLM Response
```

LLMs **do not protect you fully**.
Production apps **must** do this themselves.

---

# 8. Why companies actually do this

This exact pattern is used in:

* ChatGPT safety layers
* Customer support bots
* Education platforms
* Enterprise AI tools

Because:

* Blocking users feels bad
* Redirecting feels helpful
* Legal + ethical compliance is required

---

# 9. What this is NOT

Safety rewriting is NOT:

* Jailbreaking
* Bypassing rules
* Hiding intent

It is:

* **Responsible prompt engineering**
* **Intent redirection**

---

# 10. One-line takeaway

> **Safety rewriting means transforming unsafe user intent into a safe, educational prompt before calling the LLM.**
