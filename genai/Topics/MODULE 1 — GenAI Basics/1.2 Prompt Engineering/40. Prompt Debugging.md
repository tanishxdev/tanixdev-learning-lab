# MODULE 1.2 — Prompt Engineering

## 40. Prompt Debugging (How to Fix Broken Prompts Systematically)

Prompt debugging is **what separates beginners from professionals**.

Anyone can write prompts.
Very few can **diagnose why a prompt fails** and fix it reliably.

After this topic:

* prompt failures stop feeling random
* you know exactly what to adjust
* agents become debuggable
* iteration becomes fast and logical

---

## 1. Concept (Precise Definition)

**Prompt debugging** means:

> **Systematically identifying why a model’s output is wrong and correcting the prompt, structure, or setup to fix it.**

Not guessing.
Not rewriting randomly.
But isolating the failure cause.

---

## 2. The Core Rule (Memorize This)

When an LLM fails, **one of these is always the reason**:

1. Instruction is unclear
2. Role is missing or weak
3. Task is overloaded
4. Output format is underspecified
5. Context is too large
6. Temperature is too high
7. Wrong prompting strategy is used

There are no other reasons.

---

## 3. The Prompt Debugging Checklist (Use This Always)

When output is bad, check in this order:

1. Is the task clearly stated?
2. Is the expected output format explicit?
3. Is the role correct?
4. Is the task doing too many things?
5. Should this be multi-step?
6. Should this be few-shot?
7. Is temperature too high?
8. Is context polluted?

Never change everything at once.

---

## 4. Debugging Step 1 — Clarify the Instruction

### Broken prompt

```
Explain sorting.
```

Symptoms:

* vague output
* random depth
* inconsistent style

### Fix

```
Explain the concept of sorting in one paragraph for a beginner.
```

Instruction clarity fixes **most issues**.

---

## 5. Debugging Step 2 — Constrain the Output

### Broken prompt

```
Summarize this article.
```

Symptoms:

* too long
* too short
* inconsistent format

### Fix

```
Summarize this article in exactly 3 bullet points.
```

If output is unstable, **add constraints**.

---

## 6. Debugging Step 3 — Add or Fix Role Prompting

### Broken behavior

* tone inconsistent
* explanation level wrong
* unsafe or careless language

### Fix

Add system role:

```
You are a strict technical reviewer.
```

Roles stabilize behavior, not content.

---

## 7. Debugging Step 4 — Reduce Task Complexity

### Broken prompt

```
Read this text, extract facts, summarize them, and output JSON.
```

Symptoms:

* missed facts
* broken JSON
* hallucinations

### Fix

Split into steps:

1. extract facts
2. summarize
3. format as JSON

If one prompt contains multiple verbs, split it.

---

## 8. Debugging Step 5 — Decide Zero-Shot vs Few-Shot

If output labels are inconsistent:

* zero-shot is failing

Fix by adding **2–3 examples**.

Few-shot fixes **pattern ambiguity**, not knowledge gaps.

---

## 9. Debugging Step 6 — Control Temperature

Symptoms of high temperature:

* verbosity
* creativity
* format drift
* inconsistent answers

Fix:

```
temperature = 0
```

Especially for:

* JSON
* agents
* classification
* evaluation

---

## 10. Debugging Step 7 — Inspect Context Pollution

Common pollution sources:

1. Long chat history
2. RAG documents
3. Old instructions
4. Repeated system messages

Symptoms:

* ignoring rules
* hallucinating
* inconsistent answers

Fix by:

* trimming context
* summarizing history
* restating system rules

---

## 11. JavaScript Practice — Debugging a Broken Prompt

### Broken version

```js
import groq from "../../../src/utils/groqClient.js";

async function run() {
  const res = await groq.chat.completions.create({
    model: "llama-3.1-8b-instant",
    messages: [
      {
        role: "user",
        content: "Return JSON about recursion.",
      },
    ],
  });

  console.log(res.choices[0].message.content);
}

await run();
```

Failure reasons:

* vague task
* no schema
* no constraints
* default temperature

---

### Fixed version

```js
import groq from "../../../src/utils/groqClient.js";

async function run() {
  const prompt = `
Return ONLY valid JSON.
No extra text.
Schema:
{
  "definition": "string",
  "example": "string"
}

Topic: recursion
`;

  const res = await groq.chat.completions.create({
    model: "llama-3.1-8b-instant",
    temperature: 0,
    messages: [{ role: "user", content: prompt }],
  });

  console.log(res.choices[0].message.content);
}

await run();
```

Now:

* task is clear
* output is constrained
* behavior is stable

---

## 12. Prompt Debugging in Agents (Critical)

Agent failures usually come from:

1. unclear tool instructions
2. ambiguous outputs
3. missing JSON constraints
4. reasoning overload
5. uncontrolled retries

Prompt debugging becomes **agent debugging** later.

This is why you learned this now.

---

## 13. The Golden Debugging Rule

> **Never “improve” a prompt.
> Always fix a specific failure.**

If you cannot describe the failure in one sentence,
you don’t know what to fix yet.

---

## 14. Where You Are in the Roadmap

```
MODULE 1.2 — Prompt Engineering
31. Instruction prompting     Completed
32. Role prompting            Completed
33. Chain-of-thought          Completed
34. Zero-shot                 Completed
35. Few-shot                  Completed
36. Safety rewriting          Completed
37. JSON mode                 Completed
38. Injection prevention      Completed
39. Multi-step prompting      Completed
40. Prompt debugging          Completed
```

**MODULE 1.2 is now complete.**

---

## 15. One-Line Truth (Lock It)

> **Prompt failures are not mysterious.
> They are diagnosable and fixable.**

---

### What’s next?

You have finished GenAI Basics + Prompt Engineering.

Next major section:

### MODULE 2 — API Clients (Gemini, OpenAI, Groq)

Will move from prompt control to production-grade API engineering:

- streaming
- retries
- rate limits
- cost control
- client abstractions