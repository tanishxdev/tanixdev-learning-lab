# MODULE 1 â€” GenAI Basics (Core Mindset)

## **22. Tokenization**

This topic is **foundational**.

If tokenization is weak:

* pricing feels random
* context limits feel confusing
* latency feels mysterious
* hallucinations feel â€œmagicâ€

After this, **everything becomes predictable**.

Weâ€™ll cover:

* concept
* intuition
* internal working
* JS code (**Groq**)
* experiments
* mini project

---

## 1ï¸âƒ£ Concept (Clear & Correct Definition)

**Tokenization** is the process of:

> **Breaking raw text into smaller units called tokens that an LLM can process.**

LLMs do **not** read:

* words
* sentences
* characters

They only work with:

```
tokens â†’ numbers â†’ math
```

---

## 2ï¸âƒ£ What exactly is a Token?

A **token** can be:

* a full word
* part of a word
* punctuation
* space
* special symbol

Examples (conceptual):

```
"hello"        â†’ ["hello"]
"unbelievable" â†’ ["un", "believ", "able"]
"ChatGPT!"     â†’ ["Chat", "GPT", "!"]
```

Important truth:

> âŒ 1 word â‰  1 token

Tokens depend on the **tokenizer** used by the model.

---

## 3ï¸âƒ£ Why LLMs Use Tokens (WHY this exists)

LLMs are **neural networks**.

Neural networks understand only:

```
numbers
```

So the pipeline must be:

```
Text â†’ Tokens â†’ Token IDs â†’ Neural Network
```

Tokenization is the **bridge** between:

* human language
* machine math

---

## 4ï¸âƒ£ Internal Working (High-Level but Real)

### End-to-end flow

```
User text
  â†“
Tokenizer
  â†“
Sub-word tokens
  â†“
Token IDs (numbers)
  â†“
Transformer model
  â†“
Next-token probabilities
```

Example (illustrative):

```
"AI is powerful"
â†’ ["AI", " is", " power", "ful"]
â†’ [5023, 318, 7642, 1234]
```

After tokenization:

* words are gone
* only numbers remain

---

## 5ï¸âƒ£ Why Tokenization Affects EVERYTHING

This section is **critical**.

---

### 1ï¸âƒ£ Cost ðŸ’°

LLM APIs charge **per token**, not per word.

More tokens:

* higher cost
* faster quota exhaustion

---

### 2ï¸âƒ£ Context Window ðŸ“¦

Models have a **maximum token limit**.

What counts:

```
system instructions
+ prompt
+ chat history
+ retrieved docs (RAG)
+ model output
```

All of it is **tokens**.

---

### 3ï¸âƒ£ Latency â±ï¸

More tokens â†’

* more computation
* slower responses

---

### 4ï¸âƒ£ Truncation & Forgetting

When token limit is exceeded:

* older tokens are dropped
* important instructions vanish
* hallucinations increase

---

## 6ï¸âƒ£ Important Truth (Most People Miss This)

Two prompts with the **same meaning** can have **very different token counts**.

Example:

```
"Summarize this document"
vs
"Please provide a concise summary of the following document"
```

Second prompt:

* more tokens
* more cost
* more latency

This is why **prompt engineering exists**.

---

## 7ï¸âƒ£ JavaScript Practice â€” Observe Token Impact (Groq)

Groq doesnâ€™t expose token counts directly in responses,
but we can **observe effects through behavior**.

### Example 1: Short vs Long Prompt

```js
import groq from "../../../src/utils/groqClient.js";

async function run() {
  const shortPrompt = "Explain recursion";
  const longPrompt =
    "Please explain the concept of recursion in computer science in a very simple and beginner friendly manner with examples.";

  const shortRes = await groq.chat.completions.create({
    model: "llama-3.1-8b-instant",
    messages: [{ role: "user", content: shortPrompt }],
  });

  const longRes = await groq.chat.completions.create({
    model: "llama-3.1-8b-instant",
    messages: [{ role: "user", content: longPrompt }],
  });

  console.log("Short prompt response:\n", shortRes.choices[0].message.content);
  console.log("\nLong prompt response:\n", longRes.choices[0].message.content);
}

await run();
```

### What to observe

* Long prompt â†’ more detailed answer
* More tokens processed internally
* Higher latency

---

## 8ï¸âƒ£ Code Example â€” Token Explosion Experiment

```js
import groq from "../../../src/utils/groqClient.js";

async function run() {
  const repeatedText = "recursion ".repeat(500);

  const res = await groq.chat.completions.create({
    model: "llama-3.1-8b-instant",
    messages: [{ role: "user", content: repeatedText }],
  });

  console.log(res.choices[0].message.content);
}

await run();
```

### Insight

Even **simple repeated words** can:

* eat token budget
* push context limits
* reduce output quality

---

## 9ï¸âƒ£ Tokenization & Hallucinations (Critical Link)

When context gets too large:

* early tokens are dropped
* facts disappear
* model guesses confidently

This directly connects to:

```
Context window
RAG
Chunking
Retrieval
```

Youâ€™ll revisit this deeply in **Module 3**.

---

## ðŸ”§ Mini Practice Tasks

### Task 1

Ask the same question using:

* 5 words
* 20 words
* 50 words
  Observe output quality vs verbosity.

---

### Task 2

Paste a long article, then ask a question about the **first paragraph**.
Watch failure.

---

### Task 3

Rewrite prompts to be **shorter but clearer**.

---

## ðŸ§ª Mini Project â€” Prompt Token Optimizer (Groq)

### Goal

Train **token-awareness**, not just prompting.

---

### `22-shorten-prompt.js`

```js
import groq from "../../../src/utils/groqClient.js";

// Usage:
// node 22-shorten-prompt.js "Explain JavaScript closures with examples"

const userPrompt = process.argv.slice(2).join(" ");

if (!userPrompt) {
  console.log("Please provide a prompt to shorten.");
  process.exit(1);
}

async function main() {
  const instruction = `
Rewrite the following text to be shorter and more concise.
Do NOT change the meaning.
Do NOT add new information.
Return only the rewritten text.
`;

  const response = await groq.chat.completions.create({
    model: "llama-3.1-8b-instant",
    messages: [
      { role: "system", content: instruction },
      { role: "user", content: userPrompt },
    ],
  });

  console.log("\nOriginal Prompt:");
  console.log(userPrompt);

  console.log("\nShortened Prompt:");
  console.log(response.choices[0].message.content);
}

await main();
```

This directly builds:

* cost awareness
* context discipline
* production mindset

---

## 10ï¸âƒ£ Where This Fits in Your Roadmap

```
MODULE 1 â€” GenAI Basics
21. What is an LLM?   âœ…
22. Tokenization     â† YOU ARE HERE
23. Prompt â†’ Output cycle
```

Without tokenization:

* pricing feels random
* limits feel arbitrary
* RAG feels broken

With tokenization:

* everything becomes logical

---

## 11ï¸âƒ£ One-Line Truth (Memorize)

> **LLMs donâ€™t see words.
> They see tokens â€” and tokens decide cost, limits, and behavior.**

---
