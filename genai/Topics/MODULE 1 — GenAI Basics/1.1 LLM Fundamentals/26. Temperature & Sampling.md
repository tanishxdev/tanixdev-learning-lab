# MODULE 1 â€” GenAI Basics (Core Mindset)

## **26. Temperature & Sampling (Why LLM Outputs Change)**

This topic explains **why the same prompt never gives the same answer**
and **how you control creativity vs correctness**.

If you master this:

* hallucinations reduce
* outputs become controllable
* agents become stable
* evaluations become meaningful

---

## 1ï¸âƒ£ Concept (Precise Definition)

**Temperature & sampling** control:

> **How the model chooses the next token from the probability distribution.**

Important:

```
The model ALWAYS predicts probabilities.
Sampling decides which token is chosen.
```

---

## 2ï¸âƒ£ What the Model Actually Produces (Critical)

After reading your prompt, the model produces **this**:

```
Next token probabilities
```

Example (simplified):

```
"Recursion" â†’ 0.52
"It"        â†’ 0.21
"A"         â†’ 0.17
"Function"  â†’ 0.10
```

At this point:

âŒ No token has been chosen
âœ… Only probabilities exist

Sampling decides **what happens next**.

---

## 3ï¸âƒ£ Temperature (The Most Important Knob)

### Definition

**Temperature** controls **randomness** in token selection.

---

### Temperature scale (memorize this)

| Temperature | Behavior             |
| ----------- | -------------------- |
| 0.0         | Almost deterministic |
| 0.2         | Very focused         |
| 0.5         | Balanced             |
| 0.8         | Creative             |
| 1.0+        | Very random          |

---

### Mental model

```
Low temperature  â†’ pick high-probability tokens
High temperature â†’ explore lower-probability tokens
```

---

### Why temperature exists

Without temperature:

* answers repeat
* creativity dies
* models become boring

Too much temperature:

* nonsense
* hallucinations
* instability

---

## 4ï¸âƒ£ Sampling (Beyond Temperature)

Temperature is **not the only control**.

Sampling usually includes:

* **Top-k**
* **Top-p (nucleus sampling)**

Groq abstracts this for you, but the behavior is still there.

---

### Top-k (conceptual)

```
Keep only top K most probable tokens
Ignore the rest
```

Example:

```
Top-k = 3
Allowed tokens:
["Recursion", "It", "A"]
```

---

### Top-p (nucleus sampling)

```
Keep tokens until cumulative probability â‰¥ p
```

Example:

```
p = 0.9
Keep tokens whose probabilities add up to 90%
```

This adapts dynamically.

---

## 5ï¸âƒ£ Why Same Prompt â‰  Same Output

Now everything clicks.

Because:

```
Probability distribution
+ Sampling randomness
= different token paths
```

Even a tiny difference early in generation:

â†’ completely different sentence later.

This is called **divergence**.

---

## 6ï¸âƒ£ JavaScript Practice â€” Temperature Experiment (Groq)

### Example 1: Same prompt, different temperatures

```js
import groq from "../../../src/utils/groqClient.js";

async function run() {
  const prompt = "Explain recursion in one sentence";

  const temperatures = [0.0, 0.3, 0.7, 1.0];

  for (const temp of temperatures) {
    const res = await groq.chat.completions.create({
      model: "llama-3.1-8b-instant",
      temperature: temp,
      messages: [{ role: "user", content: prompt }],
    });

    console.log(`\nTemperature: ${temp}`);
    console.log(res.choices[0].message.content);
  }
}

await run();
```

### Observe carefully

* `0.0` â†’ rigid, repeated phrasing
* `0.3` â†’ stable and clean
* `0.7` â†’ expressive
* `1.0` â†’ risky, sometimes weird

---

## 7ï¸âƒ£ Determinism vs Creativity (Production Rule)

This rule is **industry standard**:

### Use LOW temperature when:

* RAG answers
* factual responses
* summaries
* agents making decisions
* evaluations

### Use HIGH temperature when:

* brainstorming
* writing
* ideation
* creative tasks

---

## 8ï¸âƒ£ Temperature & Hallucinations (Direct Link)

High temperature:

* increases low-probability tokens
* increases confident guessing
* increases hallucinations

This is why:

> â€œThe model sounds confident but wrong.â€

Itâ€™s not thinking â€” itâ€™s **sampling boldly**.

---

## 9ï¸âƒ£ Why Agents Prefer Low Temperature

Agents rely on:

* tool calling
* decision consistency
* repeatability

So agents typically run at:

```
temperature = 0.0 â€“ 0.3
```

Creativity breaks agents.

Stability builds agents.

---

## 10ï¸âƒ£ Common Beginner Mistake (Avoid Forever)

âŒ â€œThe model is smart today, dumb tomorrowâ€
âŒ â€œThe API is inconsistentâ€

âœ… You changed **sampling behavior**
(or didnâ€™t control it)

Always set temperature explicitly in production.

---

## ðŸ§ª Mini Project â€” Sampling Playground

### `26-temperature-playground.js`

```js
import groq from "../../../src/utils/groqClient.js";

// Usage:
// node 26-temperature-playground.js "Explain closures"

const prompt = process.argv.slice(2).join(" ");

if (!prompt) {
  console.log("Provide a prompt");
  process.exit(1);
}

async function run() {
  const temps = [0.1, 0.5, 0.9];

  for (const t of temps) {
    const res = await groq.chat.completions.create({
      model: "llama-3.1-8b-instant",
      temperature: t,
      messages: [{ role: "user", content: prompt }],
    });

    console.log(`\n--- Temperature ${t} ---`);
    console.log(res.choices[0].message.content);
  }
}

await run();
```

This builds **direct intuition**.

---

## 11ï¸âƒ£ How This Connects Forward

This topic directly affects:

* Prompt Engineering (Module 1.2)
* RAG quality (Module 3)
* Agent reliability (Module 4)
* Evaluation (Module 11)

If temperature is wrong:
everything downstream suffers.

---

## 12ï¸âƒ£ Where You Are in the Roadmap

```
MODULE 1 â€” GenAI Basics
21. What is an LLM?            âœ…
22. Tokenization              âœ…
23. Prompt â†’ Output Cycle     âœ…
24. Model Types               âœ…
25. Context Window            âœ…
26. Temperature & Sampling    â† YOU ARE HERE
27. Roles (system/user/assistant) NEXT
```

You now understand **why outputs vary**.

---

## 13ï¸âƒ£ One-Line Truth (Lock It)

> **The model predicts probabilities.
> Temperature decides how brave the model is.**
