# MODULE 1 ‚Äî GenAI Basics (Core Mindset)

## **25. Context Window (Why Models ‚ÄúForget‚Äù, Hallucinate, or Ignore You)**

If you truly understand **context window**, then:

* hallucinations stop feeling mysterious
* RAG becomes *necessary*, not fancy
* memory systems make sense
* prompt failures become debuggable

This topic connects **tokens + prompt cycle + model behavior** into one mental model.

---

## 1Ô∏è‚É£ Concept (Precise Definition)

A **context window** is:

> **The maximum number of tokens a model can read and reason over at one time ‚Äî including input and output.**

Key word: **total**.

```
Context window ‚â† prompt size
Context window = everything the model sees
```

---

## 2Ô∏è‚É£ What Exactly Counts Toward Context?

This is where most people are wrong.

Every LLM request builds this internal structure:

```
[ System instructions ]
+ [ Chat history ]
+ [ User prompt ]
+ [ Retrieved docs (RAG) ]
+ [ Model output (so far) ]
--------------------------------
= CONTEXT WINDOW
```

All of it is tokens.
All of it counts.

---

## 3Ô∏è‚É£ Groq Context Windows (Official)

From Groq docs (production models):

| Model                   | Context Window |
| ----------------------- | -------------- |
| llama-3.1-8b-instant    | 131,072 tokens |
| llama-3.3-70b-versatile | 131,072 tokens |
| openai/gpt-oss-120b     | 131,072 tokens |

This is **huge**, but **not infinite**.

---

## 4Ô∏è‚É£ Why Context Window Exists (WHY this limit is real)

LLMs are transformer models.

Transformers use **attention**, which means:

```
every token attends to every other token
```

This is **O(n¬≤)** computation.

So:

* larger context ‚Üí more memory
* more latency
* higher cost
* more instability

That‚Äôs why **hard limits exist**.

---

## 5Ô∏è‚É£ The Most Important Rule (Memorize)

> **If it‚Äôs not inside the context window, the model does not know it exists.**

This single rule explains:

* ‚ÄúWhy did it forget my instruction?‚Äù
* ‚ÄúWhy did it contradict itself?‚Äù
* ‚ÄúWhy is RAG needed?‚Äù
* ‚ÄúWhy doesn‚Äôt it remember earlier messages?‚Äù

---

## 6Ô∏è‚É£ What Happens When Context Overflows?

When total tokens exceed the limit:

### The model does NOT error immediately.

Instead, it **drops tokens**.

Usually:

```
older tokens ‚Üí dropped first
newer tokens ‚Üí kept
```

This causes:

* loss of system instructions
* forgotten constraints
* hallucinations
* confident nonsense

---

## 7Ô∏è‚É£ Practical Example ‚Äî Instruction Loss

```js
import groq from "../../../src/utils/groqClient.js";

async function run() {
  const longContext = "rules ".repeat(8000);

  const res = await groq.chat.completions.create({
    model: "llama-3.1-8b-instant",
    messages: [
      { role: "system", content: "Always answer in ONE word." },
      { role: "user", content: longContext },
      { role: "user", content: "What is recursion?" },
    ],
  });

  console.log(res.choices[0].message.content);
}

await run();
```

### Expected behavior

The model may:

* ignore ‚ÄúONE word‚Äù
* give a paragraph
* behave inconsistently

Why?

> The system instruction may have been **pushed out of context**.

---

## 8Ô∏è‚É£ Context Window vs ‚ÄúMemory‚Äù (Critical Distinction)

LLMs do **not** have memory.

What feels like memory is:

```
You resending previous messages
```

Example:

```js
messages: [
  { role: "user", content: "My name is Tanish" },
  { role: "assistant", content: "Nice to meet you, Tanish" },
  { role: "user", content: "What is my name?" }
]
```

This works only because:

> The earlier message is still **inside context**.

Once it‚Äôs pushed out ‚Üí memory is gone.

---

## 9Ô∏è‚É£ Why Long Conversations Become Useless

As chats grow:

* token count grows
* early messages drop
* reasoning degrades
* hallucinations rise

This is why **production systems never rely on raw chat history**.

Instead, they use:

```
summaries
RAG
state compression
external memory
```

(Coming in Modules 3 & 4.)

---

## üîó Context Window ‚Üí RAG (The Inevitable Link)

You now see the problem:

```
Context window is finite
Knowledge is infinite
```

So we do:

```
Store knowledge externally
‚Üì
Retrieve only relevant chunks
‚Üì
Inject into context
```

That is **RAG**.

RAG exists **because of context window limits** ‚Äî not as a fancy feature.

---

## 10Ô∏è‚É£ JavaScript Practice ‚Äî Context Stress Test

```js
import groq from "../../../src/utils/groqClient.js";

async function run() {
  const text = "JavaScript ".repeat(6000);

  const res = await groq.chat.completions.create({
    model: "llama-3.1-8b-instant",
    messages: [
      { role: "user", content: text },
      { role: "user", content: "What language was repeated?" },
    ],
  });

  console.log(res.choices[0].message.content);
}

await run();
```

### Observe

* response may be vague
* model may guess
* accuracy drops

Because **signal got drowned in tokens**.

---

## 11Ô∏è‚É£ Mini Mental Framework (Use This Always)

Before every LLM call, ask:

1. How many tokens am I sending?
2. What is most important?
3. What can be summarized?
4. What must NEVER be dropped?

This is **prompt engineering at a systems level**.

---

## 12Ô∏è‚É£ Mini Project ‚Äî Context Budget Estimator

### Goal

Build intuition for **context budgeting**.

---

### `25-context-budget.js`

```js
import groq from "../../../src/utils/groqClient.js";

// Fake example to simulate context growth

async function run() {
  let messages = [];

  for (let i = 1; i <= 10; i++) {
    messages.push({
      role: "user",
      content: `Message number ${i}: ${"data ".repeat(300)}`,
    });

    const res = await groq.chat.completions.create({
      model: "llama-3.1-8b-instant",
      messages,
    });

    console.log(`After ${i} messages:`);
    console.log(res.choices[0].message.content);
    console.log("------------------");
  }
}

await run();
```

Watch how **quality degrades** as context grows.

---

## 13Ô∏è‚É£ Where You Are in the Roadmap

```
MODULE 1 ‚Äî GenAI Basics
21. What is an LLM?            ‚úÖ
22. Tokenization              ‚úÖ
23. Prompt ‚Üí Output Cycle     ‚úÖ
24. Model Types               ‚úÖ
25. Context Window            ‚Üê YOU ARE HERE
26. Temperature & Sampling    NEXT
```

You now understand **why limits exist** and **why systems fail**.

---

## 14Ô∏è‚É£ One-Line Truth (Lock It)

> **The model cannot think about what it cannot see.
> Context window defines what it can see.**

---

When ready, say:

**‚Äú26 next ‚Äî Temperature & Sampling‚Äù**

This will explain:

* randomness
* creativity
* determinism
* why outputs change
* how hallucinations are amplified or reduced

We continue.
