# MODULE 1 — GenAI Basics (Core Mindset)

## **24. Model Types (Why Some Models Feel “Smarter” Than Others)**

This topic answers **one core confusion**:

> “Why does changing the model suddenly change quality, speed, cost, and behavior — even with the same prompt?”

After this, **model selection stops being guesswork**.

---

## 1️⃣ Concept (Precise Definition)

A **model type** defines:

> **How a model is trained, instructed, sized, and optimized — which directly controls its behavior, cost, speed, and reliability.**

When you choose a model, you are choosing:

* how it was trained
* what it is good at
* what it will fail at
* how expensive it is
* how fast it responds

---

## 2️⃣ The Three Fundamental Model Categories

All LLMs fall into **one of these three buckets**.

---

## 2.1 Base Models (Raw Predictors)

### What they are

Base models are trained to do **only one thing**:

```
predict the next token
```

They are **not trained to follow instructions**.

---

### Characteristics

* No instruction tuning
* No safety alignment
* No chat formatting
* Very unpredictable

---

### Why they exist

Base models are useful for:

* research
* fine-tuning
* embedding extraction
* internal pipelines

---

### Why YOU will not use them (now)

Because they require **extra alignment layers** to be usable.

Groq **does not expose raw base models directly** for general chat — and that’s a good thing.

---

## 2.2 Instruct Models (Most Important for You)

### What they are

Instruct models are base models that were further trained to:

> **Follow human instructions reliably**

They understand:

* “Explain”
* “Summarize”
* “Rewrite”
* “Answer clearly”

---

### Example from Groq (your default)

From Groq production models:

```
llama-3.1-8b-instant
llama-3.3-70b-versatile
openai/gpt-oss-120b
```

These are **instruction-tuned**.

---

### Why instruct models feel “smart”

Because they were trained on:

```
(prompt, ideal response)
```

pairs.

So when you write:

```
Explain recursion simply
```

They **know what “explain” means**.

---

### This is your PRIMARY model type for:

* learning
* prompting
* RAG
* agents
* tools

---

## 2.3 Chat Models (Conversation-Oriented)

### What they are

Chat models are **instruction models + conversation formatting**.

They expect:

```js
messages: [
  { role: "system", content: "..." },
  { role: "user", content: "..." }
]
```

---

### All Groq text models you’re using are chat-compatible

Example:

```js
llama-3.1-8b-instant
```

Internally, this is:

```
instruct model
+ chat wrapper
```

---

### Important truth

Chat models **do NOT have memory**.

Conversation continuity works only because:

```
You resend chat history every time
```

This is not intelligence — it’s context replay.

---

## 3️⃣ Model Size (8B vs 70B vs 120B)

Now let’s decode the **numbers** you saw in Groq docs.

---

### What “8B”, “70B”, “120B” mean

They refer to **parameter count**:

```
8B   = 8 billion parameters
70B  = 70 billion parameters
120B = 120 billion parameters
```

Parameters = learned numeric weights.

---

### Practical impact

| Size             | Behavior                            |
| ---------------- | ----------------------------------- |
| Small (8B)       | Fast, cheap, shallow                |
| Medium (32B–70B) | Balanced                            |
| Large (120B+)    | Deeper reasoning, slower, expensive |

---

### Example from Groq

| Model                     | Strength         |
| ------------------------- | ---------------- |
| `llama-3.1-8b-instant`    | Speed + learning |
| `llama-3.3-70b-versatile` | Reasoning + RAG  |
| `openai/gpt-oss-120b`     | Deep analysis    |

---

### Key insight (IMPORTANT)

> Bigger ≠ always better
> Bigger = **better pattern capacity**

For simple tasks, big models **waste money**.

---

## 4️⃣ Context Window (Why Models “Forget”)

From Groq docs:

Most production models support:

```
131,072 tokens context
```

This includes:

* system message
* user messages
* chat history
* RAG documents
* model output

---

### Why this matters

If you exceed context:

* older tokens are dropped
* instructions disappear
* hallucinations increase

This connects directly to:

```
Tokenization
Prompt → Output cycle
RAG chunking
```

---

## 5️⃣ Speed vs Quality (Groq’s Superpower)

Groq models show **token speed (t/s)**.

Example:

```
llama-3.1-8b-instant → ~560 tokens/sec
```

That’s why Groq feels **instant**.

---

### Trade-off

| Faster            | Slower           |
| ----------------- | ---------------- |
| Short reasoning   | Deeper reasoning |
| Lightweight tasks | Complex logic    |
| Cheap             | Expensive        |

This is why **agents often mix models**.

(Single model is not enough in production.)

---

## 6️⃣ Production Models vs Preview Models (DO NOT IGNORE)

From Groq docs:

### Production Models

* Stable
* SLA-backed
* Safe for real apps

Examples:

```
llama-3.1-8b-instant
llama-3.3-70b-versatile
openai/gpt-oss-120b
```

---

### Preview Models

* Experimental
* Can disappear
* Can change behavior

You use them only to:

* test
* compare
* evaluate

Never build long-term systems on preview models.

---

## 7️⃣ Special-Purpose Models (Important but Later)

Groq also exposes:

### Guard / Safety models

```
meta-llama/llama-guard-4-12b
```

Used for:

* moderation
* filtering
* policy enforcement

These are **not chat models**.

---

### Whisper (Speech-to-Text)

```
whisper-large-v3
```

Different modality entirely.

You’ll cover this in **Module 5**.

---

## 8️⃣ JavaScript Practice — Same Prompt, Different Models

This builds **model intuition**.

```js
import groq from "../../../src/utils/groqClient.js";

async function run() {
  const prompt = "Explain recursion in one sentence";

  const models = [
    "llama-3.1-8b-instant",
    "llama-3.3-70b-versatile",
  ];

  for (const model of models) {
    const res = await groq.chat.completions.create({
      model,
      messages: [{ role: "user", content: prompt }],
    });

    console.log(`\nModel: ${model}`);
    console.log(res.choices[0].message.content);
  }
}

await run();
```

### Observe

* Larger model → better phrasing
* Smaller model → faster, simpler
* Same prompt, different intelligence *feel*

---

## 9️⃣ Mini Decision Framework (Memorize This)

Ask yourself:

### 1️⃣ Is this task simple?

→ use **8B**

### 2️⃣ Needs reasoning / RAG?

→ use **70B**

### 3️⃣ High-stakes reasoning?

→ use **120B**

### 4️⃣ Need safety filtering?

→ use **Guard models**

This is how **real systems choose models**.

---

## 10️⃣ Where This Fits in Your Roadmap

```
MODULE 1 — GenAI Basics
21. What is an LLM?            ✅
22. Tokenization              ✅
23. Prompt → Output Cycle     ✅
24. Model Types               ← YOU ARE HERE
25. Context Window            NEXT
```

Now:

* token costs make sense
* speed differences make sense
* hallucinations make sense
* pricing makes sense

---

## 11️⃣ One-Line Truth (Lock It)

> **A model is not “good” or “bad” —
> it is optimized for a specific trade-off.**
