# MODULE 1 — GenAI Basics (Core Mindset)

## 30. Pricing (Why Tokens = Money)

This topic removes the last illusion:

> “LLM usage feels cheap… until the bill arrives.”

After this:

* costs become predictable
* prompt length feels expensive
* RAG design becomes economical
* agent loops stop being dangerous

---

## 1. Concept (Precise Definition)

**LLM pricing is based on tokens, not requests.**

You pay for:

```
input tokens
+ output tokens
```

Every model has **different rates** for each.

---

## 2. What You Actually Pay For

For every API call, the provider counts:

1. Tokens in system messages
2. Tokens in user messages
3. Tokens in chat history
4. Tokens in retrieved documents (RAG)
5. Tokens generated by the model

All of them are billable.

No exceptions.

---

## 3. Groq Pricing Model (Official Pattern)

From Groq docs (example models):

### llama-3.1-8b-instant

```
$0.05 per 1M input tokens
$0.08 per 1M output tokens
```

### llama-3.3-70b-versatile

```
$0.59 per 1M input tokens
$0.79 per 1M output tokens
```

### openai/gpt-oss-120b

```
$0.15 per 1M input tokens
$0.60 per 1M output tokens
```

Important insight:

> Output tokens are almost always more expensive than input tokens.

---

## 4. Why Output Tokens Cost More

Generating tokens requires:

* iterative sampling
* repeated forward passes
* higher compute per token

Input tokens are processed once.
Output tokens are generated **one-by-one**.

That’s why long answers are expensive.

---

## 5. Cost Explosion Pattern (Very Important)

Most cost spikes happen because of:

1. Long chat history
2. Uncontrolled RAG chunks
3. Verbose system prompts
4. High max output tokens
5. Agent loops

None of these are “bugs”.
They are **design decisions**.

---

## 6. Simple Cost Intuition (Memorize This)

Rule of thumb:

```
1,000 tokens ≈ 750 words
```

So:

* a 5-page document
* pasted into a prompt
* every request

= money burning quietly.

---

## 7. JavaScript Practice — Cheap vs Expensive Prompt

```js
import groq from "../../../src/utils/groqClient.js";

async function run() {
  const shortPrompt = "Summarize recursion.";
  const longPrompt =
    "Please provide a very detailed, beginner-friendly, step-by-step explanation of recursion with multiple examples and edge cases.";

  const shortRes = await groq.chat.completions.create({
    model: "llama-3.1-8b-instant",
    messages: [{ role: "user", content: shortPrompt }],
  });

  const longRes = await groq.chat.completions.create({
    model: "llama-3.1-8b-instant",
    messages: [{ role: "user", content: longPrompt }],
  });

  console.log("Short output:\n", shortRes.choices[0].message.content);
  console.log("\nLong output:\n", longRes.choices[0].message.content);
}

await run();
```

Observation:

* Same model
* Same call
* Very different token usage
* Very different cost

---

## 8. Pricing and Temperature Are Connected

Higher temperature often leads to:

* longer outputs
* more verbosity
* higher output token count

So pricing is indirectly affected by:

```
sampling settings
```

This is why production systems usually cap:

```
max_output_tokens
```

---

## 9. Pricing and Context Window (Critical Link)

Large context window ≠ free usage.

If you send:

```
50k tokens every request
```

You will pay for:

```
50k tokens every request
```

Even if the model “can” handle it.

Capability does not mean affordability.

---

## 10. Why RAG Saves Money (Not Just Accuracy)

Without RAG:

```
Paste everything
→ huge prompt
→ huge cost
```

With RAG:

```
Retrieve only relevant chunks
→ small prompt
→ controlled cost
```

RAG exists for:

1. accuracy
2. scalability
3. cost control

All three.

---

## 11. Agent Cost Trap (Very Dangerous)

Agents multiply costs because they:

* make multiple calls
* retry on failure
* loop
* call tools
* summarize results

One user action can trigger:

```
5–20 LLM calls
```

Without safeguards, this becomes a billing disaster.

---

## 12. Cost Control Techniques (Industry Standard)

Professionals always implement:

1. Max output tokens
2. Low temperature for agents
3. Prompt shortening
4. Context trimming
5. Summarization of history
6. Model routing (cheap first, expensive later)

You will implement these in later modules.

---

## 13. Mini Project — Cost Awareness Tool

### 30-cost-awareness.js

```js
import groq from "../../../src/utils/groqClient.js";

// Compare short vs long prompts mentally, not numerically

async function run() {
  const prompts = [
    "Define recursion.",
    "Explain recursion in depth with examples, edge cases, and diagrams.",
  ];

  for (const prompt of prompts) {
    const res = await groq.chat.completions.create({
      model: "llama-3.1-8b-instant",
      messages: [{ role: "user", content: prompt }],
    });

    console.log("\nPrompt:", prompt);
    console.log("Output length:", res.choices[0].message.content.length);
  }
}

await run();
```

Even without exact token counts, you’ll **feel** the cost difference.

---

## 14. Mental Pricing Framework (Use This Always)

Before every LLM call, ask:

1. How many tokens am I sending?
2. How many tokens might come back?
3. How often will this run?
4. Is a smaller model enough?
5. Can this be cached?

This mindset saves real money.

---

## 15. Where You Are in the Roadmap

```
MODULE 1 — GenAI Basics
21. What is an LLM?                 Completed
22. Tokenization                   Completed
23. Prompt → Output Cycle          Completed
24. Model Types                    Completed
25. Context Window                 Completed
26. Temperature & Sampling         Completed
27. Roles                          Completed
28. Latency vs Quality             Completed
29. Model Versions                 Completed
30. Pricing                        You are here
```

MODULE 1 is now **conceptually complete**.

---

## 16. One-Line Truth (Lock It)

> **Every extra token is a tiny charge.
> At scale, tiny charges become big bills.**
